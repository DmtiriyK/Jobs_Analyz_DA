{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa6d2782-49df-4711-942b-f2a7fe9a1d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Gathering job links...\n",
      "üîó Total URLs to fetch: 25200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gathering links: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25200/25200 [5:02:18<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü¢ Links parsed: 29100, unique: 29100\n",
      "‚úÖ Collected 29100 unique job links.\n",
      "üéâ All done! CSV –∑–∞–ø–æ–ª–Ω–µ–Ω –±–∞–∑–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import asyncio\n",
    "try:\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "except ImportError:\n",
    "    pass\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/113.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:112.0) Gecko/20100101 Firefox/112.0\",\n",
    "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 Version/16.0 Mobile/15E148 Safari/604.1\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/113.0.0.0 Edg/113.0.0.0\"\n",
    "]\n",
    "\n",
    "TIME_PARAMS = [\"r86400\", \"r604800\", \"r2592000\"]  # 1d, 7d, 30d\n",
    "GERMANY_CITIES = [\n",
    "    \"Berlin\", \"Hamburg\", \"Munich\", \"Frankfurt\", \"Cologne\", \"Stuttgart\",\n",
    "    \"Duesseldorf\", \"Leipzig\", \"Dresden\", \"Hanover\", \"Nuremberg\",\n",
    "    \"Bremen\", \"Essen\", \"Dortmund\"\n",
    "]\n",
    "KEYWORDS = [\n",
    "    \"Systemadministrator\",           # –°–∏—Å—Ç–µ–º–Ω—ã–π –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä, –ø—Ä—è–º –∫–ª–∞—Å—Å–∏–∫–∞\n",
    "    \"IT Systemadministrator\",        # IT —Å–∏—Å—Ç–µ–º–Ω—ã–π –∞–¥–º–∏–Ω\n",
    "    \"IT Administrator\",              # IT-–∞–¥–º–∏–Ω\n",
    "    \"Administrator\",                 # –ò–Ω–æ–≥–¥–∞ –ø—Ä–æ—Å—Ç–æ —Ç–∞–∫\n",
    "    \"System Engineer\",               # –°–∏—Å. –∏–Ω–∂–µ–Ω–µ—Ä\n",
    "    \"IT System Engineer\",            # IT —Å–∏—Å—Ç–µ–º–Ω—ã–π –∏–Ω–∂–µ–Ω–µ—Ä\n",
    "    \"Systembetreuer\",                # –û–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º\n",
    "    \"Netzwerkadministrator\",         # –ê–¥–º–∏–Ω —Å–µ—Ç–µ–π\n",
    "    \"Netzwerkadministratorin\",       # –ü–æ–ª–∏—Ç–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –≤–µ—Ä—Å–∏—è\n",
    "    \"Systemtechniker\",               # –°–∏—Å—Ç–µ–º–Ω—ã–π —Ç–µ—Ö–Ω–∏–∫\n",
    "    \"IT Support Administrator\",      # –°–º–µ—Å—å —Å–∞–ø–ø–æ—Ä—Ç–∞ –∏ –∞–¥–º–∏–Ω—Å—Ç–≤–∞\n",
    "    \"Serveradministrator\",           # –°–µ—Ä–≤–µ—Ä–Ω—ã–π –∞–¥–º–∏–Ω\n",
    "    \"IT Fachinformatiker Systemintegration\", # –ü–æ–ø—É–ª—è—Ä–Ω–æ –¥–ª—è –Ω–µ–º—Ü–µ–≤\n",
    "    \"IT Specialist Systemintegration\",       # –ê–Ω–≥–ª–∏–π—Å–∫–∞—è –≤–µ—Ä—Å–∏—è\n",
    "    \"Fachinformatiker Systemintegration\"     # –ü—Ä–æ—Å—Ç–æ –∏ –ø–æ–Ω—è—Ç–Ω–æ\n",
    "]\n",
    "\n",
    "CONCURRENCY_LIMIT = 20\n",
    "RETRIES = 3\n",
    "TIMEOUT = 15\n",
    "MAX_PAGES = 40\n",
    "OUTPUT_CSV = 'techsupport_germany_Big.csv.csv'\n",
    "ERROR_LOG = 'errors.log'\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=ERROR_LOG,\n",
    "    filemode='a',\n",
    "    level=logging.WARNING,\n",
    "    format='%(asctime)s %(levelname)s %(message)s'\n",
    ")\n",
    "\n",
    "def sanitize(text: str) -> str:\n",
    "    return text.replace(\" \", \"%20\") \\\n",
    "               .replace(\"√º\", \"ue\") \\\n",
    "               .replace(\"√∂\", \"oe\") \\\n",
    "               .replace(\"√§\", \"ae\") \\\n",
    "               .replace(\"√ü\", \"ss\")\n",
    "\n",
    "def run_coro(coro):\n",
    "    try:\n",
    "        return asyncio.run(coro)\n",
    "    except RuntimeError:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        return loop.run_until_complete(coro)\n",
    "\n",
    "async def fetch(session, url: str) -> str:\n",
    "    for attempt in range(1, RETRIES + 1):\n",
    "        try:\n",
    "            headers = {\"User-Agent\": random.choice(USER_AGENTS)}\n",
    "            async with session.get(url, headers=headers, timeout=TIMEOUT) as resp:\n",
    "                resp.raise_for_status()\n",
    "                return await resp.text()\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"[{attempt}/{RETRIES}] {e} @ {url}\")\n",
    "            await asyncio.sleep(2 ** attempt)\n",
    "    logging.error(f\"All retries failed for {url}\")\n",
    "    return \"\"\n",
    "\n",
    "def parse_job_cards(html: str, kw: str, city: str) -> list:\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    cards = soup.find_all('div', class_='base-card')\n",
    "    jobs = []\n",
    "    for card in cards:\n",
    "        title = (card.select_one('h3.base-search-card__title') or '').get_text(strip=True)\n",
    "        company = (card.select_one('h4.base-search-card__subtitle') or '').get_text(strip=True)\n",
    "        location = (card.select_one('span.job-search-card__location') or '').get_text(strip=True)\n",
    "        link_el = card.select_one('a.base-card__full-link')\n",
    "        link = link_el['href'] if link_el and link_el.has_attr('href') else ''\n",
    "        # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –≤—Å—ë –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–µ!\n",
    "        if title and company and location and link:\n",
    "            jobs.append({\n",
    "                'Title':    title,\n",
    "                'Company':  company,\n",
    "                'Location': location,\n",
    "                'Link':     link,\n",
    "                'Keyword':  kw,\n",
    "                'City':     city\n",
    "            })\n",
    "    return jobs\n",
    "\n",
    "async def gather_search_pages() -> list:\n",
    "    import aiohttp\n",
    "    urls = []\n",
    "    for tp in TIME_PARAMS:\n",
    "        for city in GERMANY_CITIES:\n",
    "            for kw in KEYWORDS:\n",
    "                for page in range(MAX_PAGES):\n",
    "                    start = page * 25\n",
    "                    url = (\n",
    "                        \"https://www.linkedin.com/jobs/search/\"\n",
    "                        f\"?keywords={sanitize(kw)}\"\n",
    "                        f\"&location={sanitize(city)}\"\n",
    "                        f\"&f_TPR={tp}\"\n",
    "                        f\"&start={start}\"\n",
    "                    )\n",
    "                    urls.append((url, kw, city))\n",
    "\n",
    "    print(f\"üîó Total URLs to fetch: {len(urls)}\")\n",
    "    connector = aiohttp.TCPConnector(limit=CONCURRENCY_LIMIT)\n",
    "    async with aiohttp.ClientSession(connector=connector) as session:\n",
    "        sem = asyncio.Semaphore(CONCURRENCY_LIMIT)\n",
    "        jobs = []\n",
    "\n",
    "        async def worker(u, kw, city):\n",
    "            async with sem:\n",
    "                html = await fetch(session, u)\n",
    "                if not html:\n",
    "                    logging.warning(f\"No HTML fetched for {u}\")\n",
    "                    return\n",
    "                cards = await asyncio.get_event_loop().run_in_executor(\n",
    "                    None, parse_job_cards, html, kw, city\n",
    "                )\n",
    "                jobs.extend(cards)\n",
    "\n",
    "        tasks = [worker(u, kw, city) for u, kw, city in urls]\n",
    "        for f in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=\"Gathering links\"):\n",
    "            await f\n",
    "\n",
    "    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏ –ø–æ —Å—Å—ã–ª–∫–∞–º\n",
    "    unique = {j['Link']: j for j in jobs if j['Link']}\n",
    "    print(f\"üü¢ Links parsed: {len(jobs)}, unique: {len(unique)}\")\n",
    "    return list(unique.values())\n",
    "\n",
    "def main():\n",
    "    if os.path.exists(OUTPUT_CSV):\n",
    "        os.remove(OUTPUT_CSV)\n",
    "    print(\"üîç Gathering job links...\")\n",
    "    jobs = run_coro(gather_search_pages())\n",
    "    print(f\"‚úÖ Collected {len(jobs)} unique job links.\")\n",
    "    df = pd.DataFrame(jobs)\n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(\"üéâ All done! CSV –∑–∞–ø–æ–ª–Ω–µ–Ω –±–∞–∑–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b97ad3b-a9a8-4488-b6fa-e40fae6a67f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
