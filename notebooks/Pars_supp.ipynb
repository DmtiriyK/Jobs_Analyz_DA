{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fe8bbc-806c-42a1-bca7-016e7a18c11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "jobs_list = []\n",
    "\n",
    "germany_cities = [\n",
    "    \"Germany\", \"Berlin\", \"M√ºnchen\", \"Hamburg\", \"Frankfurt\", \"K√∂ln\", \"Stuttgart\", \"D√ºsseldorf\",\n",
    "    \"Leipzig\", \"Dresden\", \"Hannover\", \"N√ºrnberg\", \"Bremen\", \"Essen\", \"Dortmund\"\n",
    "]\n",
    "\n",
    "keywords = [\n",
    "    \"Fachkraft f√ºr technischen Support\", \"Technical Support Specialist\", \"IT Support\", \"Support Specialist\",\n",
    "    \"Kundensupport\", \"User Support\", \"Supportmitarbeiter\", \"Helpdesk\", \"IT-Support\", \"IT Service Desk\",\n",
    "    \"Kundenservice\", \"Support Engineer\", \"Application Support\", \"Supportberater\", \"IT Kundenbetreuer\"\n",
    "]\n",
    "\n",
    "for city in germany_cities:\n",
    "    for kw in keywords:\n",
    "        for start in range(0, 250, 25):\n",
    "            kw_encoded = kw.replace(\" \", \"%20\").replace(\"√º\", \"u\").replace(\"√§\", \"a\").replace(\"√∂\", \"o\").replace(\"√ü\", \"ss\")\n",
    "            city_encoded = city.replace(\" \", \"%20\").replace(\"√º\", \"u\").replace(\"√§\", \"a\").replace(\"√∂\", \"o\").replace(\"√ü\", \"ss\")\n",
    "            url = f'https://www.linkedin.com/jobs/search/?keywords={kw_encoded}&location={city_encoded}&f_TPR=r2592000&start={start}'\n",
    "            print(f'üîç Scraping: {url}')\n",
    "            try:\n",
    "                response = requests.get(url, headers=headers, timeout=15)\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                job_cards = soup.find_all('div', class_='base-card')\n",
    "                print(f'   ‚û§ Found: {len(job_cards)} job listings')\n",
    "\n",
    "                for job in job_cards:\n",
    "                    title = job.find('h3', class_='base-search-card__title').text.strip() if job.find('h3', class_='base-search-card__title') else ''\n",
    "                    company = job.find('h4', class_='base-search-card__subtitle').text.strip() if job.find('h4', class_='base-search-card__subtitle') else ''\n",
    "                    location = job.find('span', class_='job-search-card__location').text.strip() if job.find('span', class_='job-search-card__location') else ''\n",
    "                    link = job.find('a', class_='base-card__full-link')['href'] if job.find('a', class_='base-card__full-link') else ''\n",
    "                    jobs_list.append({\n",
    "                        'Title': title,\n",
    "                        'Company': company,\n",
    "                        'Location': location,\n",
    "                        'Link': link,\n",
    "                        'Keyword': kw,\n",
    "                        'City': city\n",
    "                    })\n",
    "                time.sleep(random.uniform(0.7, 1.2))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"‚ùå Error:\", e)\n",
    "                time.sleep(10)\n",
    "\n",
    "print(\"üìù Fetching job descriptions...\")\n",
    "batch_size = 500\n",
    "results_batch = []\n",
    "total = len(jobs_list)\n",
    "output_file = 'raw_qa_tech_admin_germany.csv'\n",
    "\n",
    "for i, job in enumerate(jobs_list):\n",
    "    url = job['Link']\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=12)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        desc_block = soup.find('div', {'class': lambda x: x and 'description' in x})\n",
    "        if desc_block:\n",
    "            description = desc_block.get_text(separator=' ', strip=True)\n",
    "        else:\n",
    "            description = 'Not found'\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Error:\", e)\n",
    "        description = 'Error'\n",
    "\n",
    "    job['Description'] = description\n",
    "    results_batch.append(job)\n",
    "    time.sleep(random.uniform(1, 1.7))\n",
    "\n",
    "    # === –ö–∞–∂–¥—ã–µ batch_size, –∏–ª–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π ===\n",
    "    if ((i + 1) % batch_size == 0) or (i + 1 == total):\n",
    "        df = pd.DataFrame(results_batch)\n",
    "        if i < batch_size:  # –ü–µ—Ä–≤—ã–π –±–∞—Ç—á, –ø–∏—à–µ–º —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏\n",
    "            df.to_csv(output_file, index=False, mode='w')\n",
    "        else:               # –û—Å—Ç–∞–ª—å–Ω—ã–µ ‚Äî –±–µ–∑ header, –¥–æ–ø–∏—Å—ã–≤–∞–µ–º\n",
    "            df.to_csv(output_file, index=False, header=False, mode='a')\n",
    "        print(f\"üíæ Batch saved: {i + 1} / {total}\")\n",
    "        results_batch = []\n",
    "\n",
    "print(f\"‚úÖ {output_file} –≥–æ—Ç–æ–≤. –¢–µ–ø–µ—Ä—å —Ç–≤–æ–π –Ω–æ—É—Ç —Å–∫–∞–∂–µ—Ç —Å–ø–∞—Å–∏–±–æ.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
