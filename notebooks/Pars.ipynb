{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230bb42a-c094-443d-88a8-8a1b1f3086ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "jobs_list = []\n",
    "\n",
    "germany_cities = [\n",
    "    \"Germany\", \"Berlin\", \"M√ºnchen\", \"Hamburg\", \"Frankfurt\", \"K√∂ln\", \"Stuttgart\", \"D√ºsseldorf\",\n",
    "    \"Leipzig\", \"Dresden\", \"Hannover\", \"N√ºrnberg\", \"Bremen\", \"Essen\", \"Dortmund\"\n",
    "]\n",
    "\n",
    "keywords = [\n",
    "    \"Systemadministrator\", \"System Administrator\", \"IT Administrator\",\n",
    "    \"IT Systemadministrator\", \"IT System Engineer\", \"Netzwerkadministrator\",\n",
    "    \"Network Administrator\", \"IT Support Engineer\", \"IT Spezialist Systemadministration\",\n",
    "    \"Systemtechniker\"\n",
    "]\n",
    "\n",
    "for city in germany_cities:\n",
    "    for kw in keywords:\n",
    "        for start in range(0, 250, 25):\n",
    "            kw_encoded = kw.replace(\" \", \"%20\").replace(\"√º\", \"u\").replace(\"√§\", \"a\").replace(\"√∂\", \"o\").replace(\"√ü\", \"ss\")\n",
    "            city_encoded = city.replace(\" \", \"%20\").replace(\"√º\", \"u\").replace(\"√§\", \"a\").replace(\"√∂\", \"o\").replace(\"√ü\", \"ss\")\n",
    "            url = f'https://www.linkedin.com/jobs/search/?keywords={kw_encoded}&location={city_encoded}&f_TPR=r2592000&start={start}'\n",
    "            print(f'üîç Scraping: {url}')\n",
    "            try:\n",
    "                response = requests.get(url, headers=headers, timeout=15)\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                job_cards = soup.find_all('div', class_='base-card')\n",
    "                print(f'   ‚û§ Found: {len(job_cards)} job listings')\n",
    "\n",
    "                for job in job_cards:\n",
    "                    title = job.find('h3', class_='base-search-card__title').text.strip() if job.find('h3', class_='base-search-card__title') else ''\n",
    "                    company = job.find('h4', class_='base-search-card__subtitle').text.strip() if job.find('h4', class_='base-search-card__subtitle') else ''\n",
    "                    location = job.find('span', class_='job-search-card__location').text.strip() if job.find('span', class_='job-search-card__location') else ''\n",
    "                    link = job.find('a', class_='base-card__full-link')['href'] if job.find('a', class_='base-card__full-link') else ''\n",
    "                    jobs_list.append({\n",
    "                        'Title': title,\n",
    "                        'Company': company,\n",
    "                        'Location': location,\n",
    "                        'Link': link,\n",
    "                        'Keyword': kw,\n",
    "                        'City': city\n",
    "                    })\n",
    "                time.sleep(random.uniform(0.7, 1.2))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"‚ùå Error:\", e)\n",
    "                time.sleep(10)\n",
    "\n",
    "print(\"üìù Fetching job descriptions...\")\n",
    "descriptions = []\n",
    "\n",
    "for i, job in enumerate(jobs_list):\n",
    "    url = job['Link']\n",
    "    print(f\"{i+1}/{len(jobs_list)} | {url}\")\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=12)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        desc_block = soup.find('div', {'class': lambda x: x and 'description' in x})\n",
    "        if desc_block:\n",
    "            description = desc_block.get_text(separator=' ', strip=True)\n",
    "        else:\n",
    "            description = 'Not found'\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Error:\", e)\n",
    "        description = 'Error'\n",
    "    descriptions.append(description)\n",
    "    time.sleep(random.uniform(1, 1.7))\n",
    "\n",
    "df = pd.DataFrame(jobs_list)\n",
    "df['Description'] = descriptions\n",
    "\n",
    "df.to_csv('raw_systemadmin_germany.csv', index=False)\n",
    "print(\"‚úÖ raw_systemadmin_germany.csv –≥–æ—Ç–æ–≤. –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –Ω–µ –≤ —Å–ª—ë–∑—ã, –∞ –≤ batched.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
