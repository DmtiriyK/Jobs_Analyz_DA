{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41d188a1-4c8a-4a93-aa48-aa664b7faa7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Gathering job links...\n",
      "üîó Total URLs to fetch: 10080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gathering links: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10080/10080 [2:02:16<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Collected 3479 unique job links.\n",
      "üìù Fetching job descriptions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching desc 1-200: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Saved 200 jobs to raw_qa_manual_tester_germany.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching desc 201-400: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:33<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Saved 200 jobs to raw_qa_manual_tester_germany.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching desc 401-600: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:30<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Saved 200 jobs to raw_qa_manual_tester_germany.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching desc 601-800: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:23<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Saved 200 jobs to raw_qa_manual_tester_germany.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching desc 801-1000: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:24<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Saved 200 jobs to raw_qa_manual_tester_germany.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching desc 1001-1200: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:32<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Saved 200 jobs to raw_qa_manual_tester_germany.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching desc 1201-1400: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:27<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Saved 200 jobs to raw_qa_manual_tester_germany.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching desc 1401-1600: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:31<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Saved 200 jobs to raw_qa_manual_tester_germany.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching desc 1601-1800: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:40<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Saved 200 jobs to raw_qa_manual_tester_germany.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching desc 1801-2000: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:28<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Saved 200 jobs to raw_qa_manual_tester_germany.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching desc 2001-2200: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:26<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Saved 200 jobs to raw_qa_manual_tester_germany.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching desc 2201-2400: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:26<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Saved 200 jobs to raw_qa_manual_tester_germany.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching desc 2401-2600: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:42<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Saved 200 jobs to raw_qa_manual_tester_germany.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching desc 2601-2800: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:31<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Saved 200 jobs to raw_qa_manual_tester_germany.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching desc 2801-3000: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Saved 200 jobs to raw_qa_manual_tester_germany.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching desc 3001-3200: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:32<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Saved 200 jobs to raw_qa_manual_tester_germany.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching desc 3201-3400: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:28<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Saved 200 jobs to raw_qa_manual_tester_germany.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching desc 3401-3479: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [01:06<00:00,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Saved 79 jobs to raw_qa_manual_tester_germany.csv\n",
      "üéâ All done! CSV –∑–∞–ø–æ–ª–Ω–µ–Ω –¥–∞–Ω–Ω—ã–º–∏.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π LinkedIn Job Scraper —Å:\n",
    "- –†–æ—Ç–∞—Ü–∏–µ–π User-Agent\n",
    "- –¢—Ä–µ–º—è —Ñ–∏–ª—å—Ç—Ä–∞–º–∏ –ø–æ –¥–∞—Ç–µ (1–¥, 7–¥, 30–¥)\n",
    "- –°–±—Ä–æ—Å–æ–º checkpoint-–∞, —á—Ç–æ–±—ã –ø—Ä–∏ –∫–∞–∂–¥–æ–º –∑–∞–ø—É—Å–∫–µ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ —Å—Å—ã–ª–∫–∏\n",
    "- –ü–æ–¥–¥–µ—Ä–∂–∫–æ–π Jupyter (nest_asyncio) –∏ –∫–æ–Ω—Å–æ–ª–∏\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "try:\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "except ImportError:\n",
    "    pass\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import aiohttp\n",
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------------- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è -------------------------\n",
    "\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/113.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:112.0) Gecko/20100101 Firefox/112.0\",\n",
    "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 Version/16.0 Mobile/15E148 Safari/604.1\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/113.0.0.0 Edg/113.0.0.0\"\n",
    "]\n",
    "\n",
    "TIME_PARAMS = [\"r86400\", \"r604800\", \"r2592000\"]  # 1d, 7d, 30d \n",
    "GERMANY_CITIES = [\n",
    "    \"Berlin\", \"Hamburg\", \"Munich\", \"Frankfurt\", \"Cologne\", \"Stuttgart\",\n",
    "    \"Duesseldorf\", \"Leipzig\", \"Dresden\", \"Hanover\", \"Nuremberg\",\n",
    "    \"Bremen\", \"Essen\", \"Dortmund\"\n",
    "]\n",
    "KEYWORDS = [\n",
    "    \"QA Manual Tester\", \"Manual Tester\", \"Software Tester\",\n",
    "    \"QA Engineer\", \"Test Engineer\", \"Software Test Analyst\",\n",
    "    \"Manual QA Engineer\", \"Functional Tester\", \"Test Analyst\"\n",
    "]\n",
    "\n",
    "CONCURRENCY_LIMIT = 20\n",
    "RETRIES = 3\n",
    "TIMEOUT = 15       # —Å–µ–∫—É–Ω–¥ –¥–æ —Ç–∞–π–º–∞—É—Ç–∞\n",
    "BATCH_SIZE = 200\n",
    "MAX_PAGES = 40     # 40 —Å—Ç—Ä–∞–Ω–∏—Ü √ó 25 –≤–∞–∫–∞–Ω—Å–∏–π = –¥–æ 1000 URL –Ω–∞ —Ñ–∏–ª—å—Ç—Ä\n",
    "  # —Å–∫–æ–ª—å–∫–æ –≤–∞–∫–∞–Ω—Å–∏–π –±—Ä–∞—Ç—å –Ω–∞ —Ç–µ—Å—Ç\n",
    "\n",
    "OUTPUT_CSV = 'raw_qa_manual_tester_germany.csv'\n",
    "CHECKPOINT_FILE = 'checkpoint.json'\n",
    "ERROR_LOG = 'errors.log'\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=ERROR_LOG,\n",
    "    filemode='a',\n",
    "    level=logging.WARNING,\n",
    "    format='%(asctime)s %(levelname)s %(message)s'\n",
    ")\n",
    "\n",
    "# ------------------------- –£—Ç–∏–ª–∏—Ç—ã -------------------------\n",
    "\n",
    "def sanitize(text: str) -> str:\n",
    "    return text.replace(\" \", \"%20\") \\\n",
    "               .replace(\"√º\", \"ue\") \\\n",
    "               .replace(\"√∂\", \"oe\") \\\n",
    "               .replace(\"√§\", \"ae\") \\\n",
    "               .replace(\"√ü\", \"ss\")\n",
    "\n",
    "def run_coro(coro):\n",
    "    \"\"\"\n",
    "    –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –∑–∞–ø—É—Å–∫ –∫–æ—Ä—É—Ç–∏–Ω—ã –≤ Jupyter –∏–ª–∏ –≤ –∫–æ–Ω—Å–æ–ª–∏.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return asyncio.run(coro)\n",
    "    except RuntimeError:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        return loop.run_until_complete(coro)\n",
    "\n",
    "async def fetch(session: aiohttp.ClientSession, url: str) -> str:\n",
    "    \"\"\"GET-–∑–∞–ø—Ä–æ—Å —Å retry –∏ —Ä–æ—Ç–∞—Ü–∏–µ–π User-Agent.\"\"\"\n",
    "    for attempt in range(1, RETRIES + 1):\n",
    "        try:\n",
    "            headers = {\"User-Agent\": random.choice(USER_AGENTS)}\n",
    "            async with session.get(url, headers=headers, timeout=TIMEOUT) as resp:\n",
    "                resp.raise_for_status()\n",
    "                return await resp.text()\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"[{attempt}/{RETRIES}] {e} @ {url}\")\n",
    "            await asyncio.sleep(2 ** attempt)\n",
    "    logging.error(f\"All retries failed for {url}\")\n",
    "    return \"\"\n",
    "\n",
    "def parse_job_cards(html: str, kw: str, city: str) -> list:\n",
    "    \"\"\"–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ–∏—Å–∫–∞ –∫–∞—Ä—Ç–æ—á–∫–∏ –≤–∞–∫–∞–Ω—Å–∏–π.\"\"\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    cards = soup.find_all('div', class_='base-card')\n",
    "    jobs = []\n",
    "    for card in cards:\n",
    "        link_el = card.select_one('a.base-card__full-link')\n",
    "        jobs.append({\n",
    "            'Title':    (card.select_one('h3.base-search-card__title') or '').get_text(strip=True),\n",
    "            'Company':  (card.select_one('h4.base-search-card__subtitle') or '').get_text(strip=True),\n",
    "            'Location': (card.select_one('span.job-search-card__location') or '').get_text(strip=True),\n",
    "            'Link':     link_el['href'] if link_el and link_el.has_attr('href') else '',\n",
    "            'Keyword':  kw,\n",
    "            'City':     city\n",
    "        })\n",
    "    return jobs\n",
    "\n",
    "def parse_description(html: str) -> str:\n",
    "    \"\"\"–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –∏–∑ –µ—ë —Å—Ç—Ä–∞–Ω–∏—Ü—ã.\"\"\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    selectors = [\n",
    "        ('div', 'show-more-less-html__markup'),\n",
    "        ('section', 'description'),\n",
    "        ('div', 'description'),\n",
    "    ]\n",
    "    for tag, cls in selectors:\n",
    "        el = soup.find(tag, class_=cls)\n",
    "        if el:\n",
    "            return el.get_text(separator=' ', strip=True)\n",
    "    return \"Error\"\n",
    "\n",
    "def save_batch(df: pd.DataFrame, first_batch: bool):\n",
    "    \"\"\"–°–æ—Ö—Ä–∞–Ω—è–µ—Ç DataFrame –≤ CSV; –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç –ø—É—Å—Ç—ã–µ –ø–∞—á–∫–∏.\"\"\"\n",
    "    if df.empty:\n",
    "        print(\"‚ö†Ô∏è Empty batch ‚Äî skipping save.\")\n",
    "        return\n",
    "    mode = 'w' if first_batch else 'a'\n",
    "    header = first_batch\n",
    "    df.to_csv(OUTPUT_CSV, index=False, mode=mode, header=header)\n",
    "    print(f\"‚úîÔ∏è Saved {len(df)} jobs to {OUTPUT_CSV}\")\n",
    "\n",
    "# ------------------------- –≠—Ç–∞–ø 1: —Å–±–æ—Ä —Å—Å—ã–ª–æ–∫ -------------------------\n",
    "\n",
    "async def gather_search_pages() -> list:\n",
    "    urls = []\n",
    "    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—Å–µ—Ö URL\n",
    "    for tp in TIME_PARAMS:\n",
    "        for city in GERMANY_CITIES:\n",
    "            for kw in KEYWORDS:\n",
    "                for page in range(MAX_PAGES):\n",
    "                    start = page * 25\n",
    "                    url = (\n",
    "                        \"https://www.linkedin.com/jobs/search/\"\n",
    "                        f\"?keywords={sanitize(kw)}\"\n",
    "                        f\"&location={sanitize(city)}\"\n",
    "                        f\"&f_TPR={tp}\"\n",
    "                        f\"&start={start}\"\n",
    "                    )\n",
    "                    urls.append((url, kw, city))\n",
    "\n",
    "    print(f\"üîó Total URLs to fetch: {len(urls)}\")  # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å >> 12\n",
    "\n",
    "    connector = aiohttp.TCPConnector(limit=CONCURRENCY_LIMIT)\n",
    "    async with aiohttp.ClientSession(connector=connector) as session:\n",
    "        sem = asyncio.Semaphore(CONCURRENCY_LIMIT)\n",
    "        jobs = []\n",
    "\n",
    "        async def worker(u, kw, city):\n",
    "            async with sem:\n",
    "                html = await fetch(session, u)\n",
    "                if not html:\n",
    "                    return\n",
    "                cards = await asyncio.get_event_loop().run_in_executor(\n",
    "                    None, parse_job_cards, html, kw, city\n",
    "                )\n",
    "                jobs.extend(cards)\n",
    "\n",
    "        tasks = [worker(u, kw, city) for u, kw, city in urls]\n",
    "        for f in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=\"Gathering links\"):\n",
    "            await f\n",
    "\n",
    "    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL\n",
    "    unique = {j['Link']: j for j in jobs if j['Link']}\n",
    "    return list(unique.values())\n",
    "\n",
    "# ------------------------- –≠—Ç–∞–ø 2: —Å–±–æ—Ä –æ–ø–∏—Å–∞–Ω–∏–π -------------------------\n",
    "\n",
    "async def gather_descriptions(jobs: list):\n",
    "    connector = aiohttp.TCPConnector(limit=CONCURRENCY_LIMIT)\n",
    "    executor = ThreadPoolExecutor(max_workers=CONCURRENCY_LIMIT)\n",
    "\n",
    "    async with aiohttp.ClientSession(connector=connector) as session:\n",
    "        sem = asyncio.Semaphore(CONCURRENCY_LIMIT)\n",
    "        first_batch = True\n",
    "\n",
    "        # –†–∞–∑–±–∏–≤–∞–µ–º —Å–ø–∏—Å–æ–∫ –Ω–∞ –±–∞—Ç—á–∏\n",
    "        for start in range(0, len(jobs), BATCH_SIZE):\n",
    "            end = min(start + BATCH_SIZE, len(jobs))\n",
    "            batch = jobs[start:end]\n",
    "\n",
    "            # –û–¥–Ω–∞ –≤–æ—Ä–∫-—Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ job\n",
    "            async def worker(job):\n",
    "                async with sem:\n",
    "                    html = await fetch(session, job['Link'])\n",
    "                    job['ParsedDate'] = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "                    if not html:\n",
    "                        job.update({\n",
    "                            'Description': 'Error',\n",
    "                            'PostedDate': None,\n",
    "                            'EmploymentType': None,\n",
    "                            'SeniorityLevel': None,\n",
    "                            'Skills': None,\n",
    "                            'Industries': None,\n",
    "                            'JobFunction': None,\n",
    "                            'RemoteStatus': None,\n",
    "                            'Salary': None\n",
    "                        })\n",
    "                        return\n",
    "\n",
    "                    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "                    # 1) JSON-LD\n",
    "                    ld = {}\n",
    "                    tag = soup.find('script', {'type': 'application/ld+json'})\n",
    "                    if tag:\n",
    "                        try:\n",
    "                            ld = json.loads(tag.string)\n",
    "                        except:\n",
    "                            ld = {}\n",
    "\n",
    "                    # 2) Guest API\n",
    "                    api_data = {}\n",
    "                    if not ld.get('skills') or not ld.get('experienceRequirements'):\n",
    "                        m = re.search(r'jobId=(\\d+)', job['Link'])\n",
    "                        if m:\n",
    "                            jid = m.group(1)\n",
    "                            url = f'https://www.linkedin.com/jobs-guest/jobs/api/jobDetails?jobId={jid}'\n",
    "                            resp = await session.get(url)\n",
    "                            if resp.status == 200:\n",
    "                                try:\n",
    "                                    api_data = await resp.json()\n",
    "                                except:\n",
    "                                    api_data = {}\n",
    "\n",
    "                    # 3) –ó–∞–¥–µ–∑–∫—Ä–∏–ø—à–Ω\n",
    "                    job['Description'] = await asyncio.get_event_loop() \\\n",
    "                        .run_in_executor(executor, parse_description, html)\n",
    "\n",
    "                    # 4) –ù–∞–ø–æ–ª–Ω—è–µ–º –ø–æ–ª—è\n",
    "                    job['PostedDate'] = extract_posted_date(soup)\n",
    "\n",
    "                    job['EmploymentType'] = (\n",
    "                        ld.get('employmentType')\n",
    "                        or api_data.get('employmentType')\n",
    "                        or extract_employment_type(soup)\n",
    "                    )\n",
    "\n",
    "                    job['SeniorityLevel'] = (\n",
    "                        ld.get('experienceRequirements')\n",
    "                        or api_data.get('experienceRequirements')\n",
    "                        or extract_seniority_level(soup)\n",
    "                    )\n",
    "\n",
    "                    # Skills\n",
    "                    skills = ld.get('skills') or api_data.get('skills')\n",
    "                    if isinstance(skills, list):\n",
    "                        job['Skills'] = ', '.join(skills)\n",
    "                    else:\n",
    "                        job['Skills'] = skills or extract_skills(soup)\n",
    "\n",
    "                    job['Industries'] = (\n",
    "                        ld.get('industry')\n",
    "                        or api_data.get('industry')\n",
    "                        or extract_industries(soup)\n",
    "                    )\n",
    "\n",
    "                    job['JobFunction'] = (\n",
    "                        ld.get('jobFunction')\n",
    "                        or api_data.get('jobFunction')\n",
    "                        or extract_job_function(soup)\n",
    "                    )\n",
    "\n",
    "                    job['RemoteStatus'] = (\n",
    "                        ld.get('workRemoteInstructions')\n",
    "                        or api_data.get('workRemoteInstructions')\n",
    "                        or extract_remote_status(soup)\n",
    "                    )\n",
    "\n",
    "                    # Salary\n",
    "                    salary = (ld.get('baseSalary') or {}).get('value') \\\n",
    "                             or api_data.get('salary')\n",
    "                    job['Salary'] = salary or extract_salary(soup)\n",
    "\n",
    "            # –°–æ–±–∏—Ä–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –∫–æ—Ä—É—Ç–∏–Ω—ã\n",
    "            tasks = [worker(j) for j in batch]\n",
    "            for f in tqdm(\n",
    "                asyncio.as_completed(tasks),\n",
    "                total=len(tasks),\n",
    "                desc=f\"Fetching desc {start+1}-{end}\"\n",
    "            ):\n",
    "                await f\n",
    "\n",
    "            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞—Ç—á –≤ CSV\n",
    "            df = pd.DataFrame(batch)\n",
    "            save_batch(df, first_batch)\n",
    "            first_batch = False\n",
    "\n",
    "        executor.shutdown()\n",
    "\n",
    "def extract_seniority_level(soup):\n",
    "    text = soup.get_text().lower()\n",
    "    if 'junior' in text:\n",
    "        return 'Junior'\n",
    "    elif 'mid' in text or 'mittel' in text:\n",
    "        return 'Mid'\n",
    "    elif 'senior' in text or 'leitung' in text:\n",
    "        return 'Senior'\n",
    "    elif re.search(r'\\b(6|7|8)\\s*\\+\\s*jahre\\b', text):\n",
    "        return 'Senior'\n",
    "    return 'Unknown'\n",
    "\n",
    "def extract_skills(soup):\n",
    "    text = soup.get_text().lower()\n",
    "    keywords = ['python', 'selenium', 'jira', 'sql', 'sap', 'test automation', 'manual testing', 'erp']\n",
    "    found = [kw.capitalize() for kw in keywords if kw in text]\n",
    "    return ', '.join(found) if found else None\n",
    "\n",
    "def extract_employment_type(soup):\n",
    "    text = soup.get_text().lower()\n",
    "    if 'vollzeit' in text or 'full-time' in text:\n",
    "        return 'Full-time'\n",
    "    elif 'teilzeit' in text or 'part-time' in text:\n",
    "        return 'Part-time'\n",
    "    return 'Unknown'\n",
    "\n",
    "def extract_remote_status(soup):\n",
    "    text = soup.get_text().lower()\n",
    "    if 'remote' in text:\n",
    "        return 'Remote'\n",
    "    elif 'hybrid' in text:\n",
    "        return 'Hybrid'\n",
    "    elif 'vor ort' in text or 'on-site' in text:\n",
    "        return 'On-site'\n",
    "    return 'Unknown'\n",
    "\n",
    "def extract_salary(soup):\n",
    "    text = soup.get_text()\n",
    "    matches = re.findall(r'‚Ç¨\\s*\\d+[.,]?\\d*', text)\n",
    "    return matches[0] if matches else None\n",
    "\n",
    "def extract_posted_date(soup):\n",
    "    tag = soup.find('span', class_='posted-time-ago__text')\n",
    "    return tag.text.strip() if tag else None\n",
    "\n",
    "def extract_industries(soup):\n",
    "    text = soup.get_text().lower()\n",
    "    for keyword in ['it', 'software', 'telekommunikation', 'energie', 'logistik']:\n",
    "        if keyword in text:\n",
    "            return keyword.capitalize()\n",
    "    return None\n",
    "\n",
    "def extract_job_function(soup):\n",
    "    text = soup.get_text().lower()\n",
    "    for keyword in ['test', 'entwicklung', 'analyse', 'support']:\n",
    "        if keyword in text:\n",
    "            return keyword.capitalize()\n",
    "    return None\n",
    "\n",
    "\n",
    "# ------------------------- –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è -------------------------\n",
    "\n",
    "def main():\n",
    "    # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ checkpoint, —á—Ç–æ–±—ã —Å—Ç–∞—Ä—Ç–æ–≤–∞—Ç—å —Å —á–∏—Å—Ç–æ–≥–æ –ª–∏—Å—Ç–∞\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        os.remove(CHECKPOINT_FILE)\n",
    "    if os.path.exists(OUTPUT_CSV):\n",
    "        os.remove(OUTPUT_CSV)\n",
    "\n",
    "    print(\"üîç Gathering job links...\")\n",
    "    jobs = run_coro(gather_search_pages())\n",
    "    print(f\"‚úÖ Collected {len(jobs)} unique job links.\")\n",
    "\n",
    "    print(\"üìù Fetching job descriptions...\")\n",
    "    run_coro(gather_descriptions(jobs))\n",
    "\n",
    "    print(\"üéâ All done! CSV –∑–∞–ø–æ–ª–Ω–µ–Ω –¥–∞–Ω–Ω—ã–º–∏.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c795cae8-82b2-4cac-8306-bdce693120f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
